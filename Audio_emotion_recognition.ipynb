{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc04f7a6ee9c4808ba0828820b23a8af": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3dfb746bad1648229a483afbcf65eacd",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠙\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠙</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3dfb746bad1648229a483afbcf65eacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Download data"
      ],
      "metadata": {
        "id": "GoTQGo39_dp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLSqow667jtM",
        "outputId": "11314d41-5e6f-4b09-e757-a8762b78d48a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PcfHs607Mv3",
        "outputId": "cad72bea-4219-456b-ac3f-7afcb1853ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "LJPWBmrd7nIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "w63qW95K7u9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pXwS63TB9HTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeBnBPeb9KOP",
        "outputId": "96220208-4554-47b2-9a58-69e03d5c3823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ejlok1/cremad?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 451M/451M [00:04<00:00, 108MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ejlok1/cremad/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3QwHDOZ9d-C",
        "outputId": "3486772a-64c4-4011-e2ec-83eef014a661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ejlok1/surrey-audiovisual-expressed-emotion-savee\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ita4o5Iz9qQQ",
        "outputId": "b8b98743-ac1e-4a38-c66f-6d4504879a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ejlok1/surrey-audiovisual-expressed-emotion-savee?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 107M/107M [00:00<00:00, 135MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6y-1x5O9yB5",
        "outputId": "31c44731-d3ae-4c18-862e-8876dee0a71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/toronto-emotional-speech-set-tess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/root/.cache/kagglehub/datasets/ejlok1/cremad/versions/1\" \"/content/drive/MyDrive/Colab Notebooks/cremad\"\n",
        "!cp -r \"/root/.cache/kagglehub/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee/versions/1\" \"/content/drive/MyDrive/Colab Notebooks/savee\"\n",
        "!cp -r \"/kaggle/input/ravdess-emotional-speech-audio\" \"/content/drive/MyDrive/Colab Notebooks/ravdess\"\n",
        "!cp -r \"/kaggle/input/toronto-emotional-speech-set-tess\" \"/content/drive/MyDrive/Colab Notebooks/tess\"\n"
      ],
      "metadata": {
        "id": "8H--mVaf-Yqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load data"
      ],
      "metadata": {
        "id": "-Xi7CfxR_gXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audiomentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3I0ZhAGNJA8",
        "outputId": "4afa81ab-eb7d-4cb6-a9a8-639e84a56433"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting audiomentations\n",
            "  Downloading audiomentations-0.41.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (2.0.2)\n",
            "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
            "  Downloading numpy_minmax-0.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
            "  Downloading numpy_rms-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting librosa!=0.10.0,<0.11.0,>=0.8.0 (from audiomentations)\n",
            "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting python-stretch<1,>=0.3.1 (from audiomentations)\n",
            "  Downloading python_stretch-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: scipy<2,>=1.4 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (1.15.3)\n",
            "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from audiomentations) (0.5.0.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.14.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2025.6.15)\n",
            "Downloading audiomentations-0.41.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.9/85.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy_minmax-0.4.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading numpy_rms-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17 kB)\n",
            "Downloading python_stretch-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-stretch, numpy-rms, numpy-minmax, librosa, audiomentations\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.11.0\n",
            "    Uninstalling librosa-0.11.0:\n",
            "      Successfully uninstalled librosa-0.11.0\n",
            "Successfully installed audiomentations-0.41.0 librosa-0.10.2.post1 numpy-minmax-0.4.0 numpy-rms-0.5.0 python-stretch-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Audio\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "UbtO2hnq_idR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ravdess = \"/content/drive/MyDrive/Colab Notebooks/ravdess/audio_speech_actors_01-24\"\n",
        "Crema   = \"/content/drive/MyDrive/Colab Notebooks/cremad/AudioWAV\"\n",
        "Tess    = \"/content/drive/MyDrive/Colab Notebooks/tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data\"\n",
        "Savee   = \"/content/drive/MyDrive/Colab Notebooks/savee/ALL\""
      ],
      "metadata": {
        "id": "JPPnyl4AAJIP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in ravdess_directory_list:\n",
        "    actor_folder = os.path.join(Ravdess, dir)\n",
        "    if not os.path.isdir(actor_folder):\n",
        "        continue\n",
        "    for file in os.listdir(actor_folder):\n",
        "        if not file.endswith(\".wav\"):\n",
        "            continue\n",
        "        part = file.split('.')[0].split('-')\n",
        "        file_emotion.append(int(part[2]))\n",
        "        file_path.append(os.path.join(actor_folder, file))\n",
        "\n",
        "Ravdess_df = pd.DataFrame({\n",
        "    \"Emotions\": file_emotion,\n",
        "    \"Path\": file_path\n",
        "})\n",
        "\n",
        "Ravdess_df.Emotions.replace({\n",
        "    1: 'neutral',\n",
        "    2: 'calm',\n",
        "    3: 'happy',\n",
        "    4: 'sad',\n",
        "    5: 'angry',\n",
        "    6: 'fear',\n",
        "    7: 'disgust',\n",
        "    8: 'surprise'\n",
        "}, inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEhzJv30BT_J",
        "outputId": "cc281786-d617-4604-ec20-4edf43870f18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-6-2243743617.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  Ravdess_df.Emotions.replace({\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crema_directory_list = os.listdir(Crema)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in crema_directory_list:\n",
        "    if not file.endswith(\".wav\"):\n",
        "        continue\n",
        "    full_path = os.path.join(Crema, file)\n",
        "    file_path.append(full_path)\n",
        "    part = file.split('_')\n",
        "    emotion_code = part[2]\n",
        "    file_emotion.append({\n",
        "        'SAD': 'sad',\n",
        "        'ANG': 'angry',\n",
        "        'DIS': 'disgust',\n",
        "        'FEA': 'fear',\n",
        "        'HAP': 'happy',\n",
        "        'NEU': 'neutral'\n",
        "    }.get(emotion_code, 'Unknown'))\n",
        "\n",
        "Crema_df = pd.DataFrame({\n",
        "    \"Emotions\": file_emotion,\n",
        "    \"Path\": file_path\n",
        "})\n"
      ],
      "metadata": {
        "id": "vZXj9-KMCA4N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tess_directory_list = os.listdir(Tess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in tess_directory_list:\n",
        "    subdir_path = os.path.join(Tess, dir)\n",
        "    if not os.path.isdir(subdir_path):\n",
        "        continue\n",
        "    for file in os.listdir(subdir_path):\n",
        "        if not file.endswith(\".wav\"):\n",
        "            continue\n",
        "        part = file.split('.')[0].split('_')[2]\n",
        "        emotion = 'surprise' if part == 'ps' else part\n",
        "        file_emotion.append(emotion.lower())\n",
        "        file_path.append(os.path.join(subdir_path, file))\n",
        "\n",
        "Tess_df = pd.DataFrame({\n",
        "    \"Emotions\": file_emotion,\n",
        "    \"Path\": file_path\n",
        "})\n"
      ],
      "metadata": {
        "id": "Nk-yw7MqCTfU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savee_directory_list = os.listdir(Savee)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in savee_directory_list:\n",
        "    if not file.endswith(\".wav\"):\n",
        "        continue\n",
        "    full_path = os.path.join(Savee, file)\n",
        "    file_path.append(full_path)\n",
        "    part = file.split('_')[1]\n",
        "    ele = part[:-6]\n",
        "    emotion = {\n",
        "        'a': 'angry',\n",
        "        'd': 'disgust',\n",
        "        'f': 'fear',\n",
        "        'h': 'happy',\n",
        "        'n': 'neutral',\n",
        "        'sa': 'sad'\n",
        "    }.get(ele, 'surprise')\n",
        "    file_emotion.append(emotion)\n",
        "\n",
        "Savee_df = pd.DataFrame({\n",
        "    \"Emotions\": file_emotion,\n",
        "    \"Path\": file_path\n",
        "})\n"
      ],
      "metadata": {
        "id": "IaCkW37eCocS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Check for existence\n",
        "valid_count = sum(os.path.exists(p) for p in data_path['Path'])\n",
        "print(f\"TOTAL valid paths: {valid_count} / {len(data_path)}\")\n",
        "\n",
        "# Save\n",
        "data_path.to_csv(\"/content/drive/MyDrive/Model_Results/data_path.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jStDOki9G4B0",
        "outputId": "cf6b211a-e2f2-48e5-b141-a118267a98a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOTAL valid paths: 12162 / 12162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path[\"Emotions\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "dds07ZxLC4rr",
        "outputId": "014d5b9c-4da5-4fa7-ecee-15928d6727c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Emotions\n",
              "happy       1923\n",
              "sad         1923\n",
              "fear        1923\n",
              "angry       1923\n",
              "disgust     1923\n",
              "neutral     1703\n",
              "surprise     652\n",
              "calm         192\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Emotions</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>happy</th>\n",
              "      <td>1923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sad</th>\n",
              "      <td>1923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fear</th>\n",
              "      <td>1923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>angry</th>\n",
              "      <td>1923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>disgust</th>\n",
              "      <td>1923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>1703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surprise</th>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>calm</th>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data, sample_rate, frame_length=2048, hop_length=512, offset=0.6, duration=2.5):\n",
        "    \"\"\"\n",
        "    Extract hand-crafted features: ZCR, RMSE, Chroma STFT, and MFCC\n",
        "    \"\"\"\n",
        "    # Zero Crossing Rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(data, frame_length=frame_length, hop_length=hop_length)\n",
        "    zcr = np.squeeze(zcr)\n",
        "\n",
        "    # Root Mean Square Energy\n",
        "    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n",
        "    rmse = np.squeeze(rmse)\n",
        "\n",
        "    # MFCC features (13 coefficients as commonly used)\n",
        "    mfcc = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13, hop_length=hop_length)\n",
        "    mfcc = mfcc.flatten()\n",
        "\n",
        "    # Chroma STFT features (12 pitch classes)\n",
        "    chroma = librosa.feature.chroma_stft(y=data, sr=sample_rate, hop_length=hop_length)\n",
        "    chroma = chroma.flatten()\n",
        "\n",
        "    # Concatenate all features horizontally\n",
        "    features = np.concatenate([zcr, rmse, mfcc, chroma])\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def noise_augmentation(data, noise_factor=0.035):\n",
        "    \"\"\"\n",
        "    Add Gaussian noise to audio data\n",
        "    \"\"\"\n",
        "    noise = np.random.normal(0, 1, len(data))\n",
        "    # Scale noise and add to data\n",
        "    augmented_data = data + noise_factor * noise\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "def pitch_shift_augmentation(data, sample_rate, pitch_factor=0.7):\n",
        "    \"\"\"\n",
        "    Apply pitch shifting to audio data\n",
        "    \"\"\"\n",
        "    # Using librosa's pitch_shift with the specified factor\n",
        "    # Converting pitch_factor to semitones (0.7 factor ≈ -5.9 semitones)\n",
        "    semitones = 12 * np.log2(pitch_factor)\n",
        "    augmented_data = librosa.effects.pitch_shift(data, sr=sample_rate, n_steps=semitones)\n",
        "    return augmented_data\n",
        "\n",
        "\n",
        "def load_and_extract_features(file_path, sample_rate=22050, offset=0.6, duration=2.5):\n",
        "    \"\"\"\n",
        "    Load audio file and extract features:\n",
        "    1. Original audio (O.A.)\n",
        "    2. Noise-augmented audio (N.A.)\n",
        "    3. Pitch-shifted audio (P.A.)\n",
        "    4. Combined noise and pitch-shifted audio (N.A. + P.A.)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load original audio\n",
        "        data, _ = librosa.load(file_path, sr=sample_rate, offset=offset, duration=duration)\n",
        "\n",
        "        # PADDING FIX: Ensure consistent length\n",
        "        expected_length = int(sample_rate * duration)  # 22050 * 2.5 = 55125 samples\n",
        "\n",
        "        if len(data) < expected_length:\n",
        "            # Pad with zeros if audio is shorter than expected\n",
        "            data = np.pad(data, (0, expected_length - len(data)), mode='constant', constant_values=0)\n",
        "        elif len(data) > expected_length:\n",
        "            # Truncate if audio is longer than expected (shouldn't happen with duration param, but safety)\n",
        "            data = data[:expected_length]\n",
        "\n",
        "        # Create augmented versions\n",
        "        noise_data = noise_augmentation(data, noise_factor=0.035)\n",
        "        pitch_data = pitch_shift_augmentation(data, sample_rate, pitch_factor=0.7)\n",
        "        combined_data = pitch_shift_augmentation(noise_data, sample_rate, pitch_factor=0.7)\n",
        "\n",
        "        # Extract features from all versions\n",
        "        original_features = extract_features(data, sample_rate)\n",
        "        noise_features = extract_features(noise_data, sample_rate)\n",
        "        pitch_features = extract_features(pitch_data, sample_rate)\n",
        "        combined_features = extract_features(combined_data, sample_rate)\n",
        "\n",
        "        # Concatenate all features (stacking as mentioned in paper)\n",
        "        final_features = np.concatenate([\n",
        "            original_features,    # O.A.\n",
        "            noise_features,       # N.A.\n",
        "            pitch_features,       # P.A.\n",
        "            combined_features     # N.A. + P.A.\n",
        "        ])\n",
        "\n",
        "        return final_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "TTFJL74PQoH9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"GPU Count:\", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHutRfFI659O",
        "outputId": "00bedd48-ddbd-4f72-f665-f6e593015adb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU Count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features_safe(file_path, emotion):\n",
        "    \"\"\"Safe wrapper for feature extraction\"\"\"\n",
        "    try:\n",
        "        features = load_and_extract_features(file_path)\n",
        "        return features, emotion if features is not None else (None, None)\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "# Prepare data\n",
        "data_subset = data_path\n",
        "file_paths = data_subset['Path'].tolist()\n",
        "emotions = data_subset['Emotions'].tolist()\n",
        "\n",
        "# Parallel processing\n",
        "n_jobs = -1  # Use all available cores\n",
        "print(\"Extracting features in parallel...\")\n",
        "\n",
        "results = Parallel(n_jobs=n_jobs)(\n",
        "    delayed(extract_features_safe)(path, emotion)\n",
        "    for path, emotion in tqdm(zip(file_paths, emotions), total=len(file_paths))\n",
        ")\n",
        "\n",
        "# Collect valid results\n",
        "X = []\n",
        "y = []\n",
        "for features, emotion in results:\n",
        "    if features is not None:\n",
        "        X.append(features)\n",
        "        y.append(emotion)\n",
        "\n",
        "print(f\"Successfully processed {len(X)} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STc-18qGVUJu",
        "outputId": "e77083a7-ff63-40c5-9ab6-16591c69633a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12162/12162 [54:38<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 12162 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.array(X)\n",
        "y1 = np.array(y)"
      ],
      "metadata": {
        "id": "mih_Ni8RVv6b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/drive/MyDrive/Model_Results'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save the arrays\n",
        "np.save(f'{save_dir}/X_features.npy', X1)\n",
        "np.save(f'{save_dir}/y_labels.npy', y1)"
      ],
      "metadata": {
        "id": "_Ajj7VKcQpE-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To load later:\n",
        "X = np.load('/content/drive/MyDrive/Model_Results/X_features.npy')\n",
        "y = np.load('/content/drive/MyDrive/Model_Results/y_labels.npy')"
      ],
      "metadata": {
        "id": "zAC_YBjTX-Iw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.nan_to_num(X, nan=0.0)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "uNyjuoS5XWA_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "print(f\"Emotion classes: {le.classes_}\")\n",
        "print(f\"Number of classes: {len(le.classes_)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBLdb0URXk5f",
        "outputId": "3ef77a5b-59cd-4511-c02a-86209d568793"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion classes: ['angry' 'calm' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
            "Number of classes: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data (80% train, 10% validation, 10% test)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_scaled, y_categorical, test_size=0.1, random_state=42, stratify=y_categorical\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.111, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "889x3Zl5XpG6",
        "outputId": "0391e91f-3770-4195-c354-67e9af3a0d5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 9730 samples\n",
            "Validation set: 1215 samples\n",
            "Test set: 1217 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare input shape for 1D CNN\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "num_classes = y_categorical.shape[1]\n",
        "\n",
        "print(f\"Input shape: {input_shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP1uUG43bpCu",
        "outputId": "08842aa6-524d-445f-bf01-21c25612b9a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (11664, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM, Bidirectional\n",
        "\n",
        "# Build 1D CNN Model (Table 2 from paper)\n",
        "def build_1d_cnn_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    1D CNN model exactly as described in paper's Table 2\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Conv Block 1 (no dropout)\n",
        "    model.add(Conv1D(128, kernel_size=5, activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Conv Block 2 (with dropout)\n",
        "    model.add(Conv1D(128, kernel_size=5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Block 3 (no dropout)\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Conv Block 4 (with dropout)\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Block 5 (with dropout)\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build CNN_Bi-LSTM Model (Table 2 from paper)\n",
        "def build_cnn_bilstm_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    CNN_Bi-LSTM model exactly as described in paper's Table 2\n",
        "    The only difference from CNN model is the addition of Bi-LSTM layer\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Conv Block 1 (no dropout)\n",
        "    model.add(Conv1D(128, kernel_size=5, activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Conv Block 2 (with dropout)\n",
        "    model.add(Conv1D(128, kernel_size=5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Conv Block 3 (no dropout)\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "    # Conv Block 4 (with dropout)\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # *** Bi-directional LSTM layer ***\n",
        "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "\n",
        "    # Conv Block 5 (with dropout)\n",
        "    model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Dense layers\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JVBZ810Bv9HE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build both models\n",
        "cnn_model = build_1d_cnn_model(input_shape, num_classes)\n",
        "bilstm_model = build_cnn_bilstm_model(input_shape, num_classes)\n",
        "\n",
        "# Model summaries\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"1D CNN MODEL SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "cnn_model.summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CNN_Bi-LSTM MODEL SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "bilstm_model.summary()\n",
        "\n",
        "# Compile models with paper's exact parameters\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "bilstm_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K1jsUT-wwEuC",
        "outputId": "8a0a9da8-b554-4f22-b6ed-011f5b80febb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "1D CNN MODEL SUMMARY\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11664\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │           \u001b[38;5;34m768\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11664\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5832\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5832\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m82,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5832\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m24,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1458\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1458\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m12,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1458\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m6,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11648\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,491,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11664</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11664</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5832</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5832</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5832</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1458</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1458</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1458</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11648</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,491,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,620,264\u001b[0m (6.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,620,264</span> (6.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,619,176\u001b[0m (6.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,619,176</span> (6.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,088\u001b[0m (4.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> (4.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "CNN_Bi-LSTM MODEL SUMMARY\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11664\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │           \u001b[38;5;34m768\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11664\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5832\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5832\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m82,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5832\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_6 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m24,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2916\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_7 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1458\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1458\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m12,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1458\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_8 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m66,048\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m12,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m729\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m364\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11648\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,491,072\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11664</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11664</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5832</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5832</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5832</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2916</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1458</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1458</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1458</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">729</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">364</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11648</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,491,072</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,692,456\u001b[0m (6.46 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,692,456</span> (6.46 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,691,368\u001b[0m (6.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,691,368</span> (6.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,088\u001b[0m (4.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> (4.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow dagshub\n",
        "\n",
        "import mlflow\n",
        "import dagshub\n",
        "\n",
        "# Initialize DagsHub (replace with your actual repo details)\n",
        "dagshub.init(repo_owner=\"dimna21\", repo_name=\"Emotion_Recognition\", mlflow=True)\n",
        "\n",
        "# Set MLflow tracking URI to DagsHub\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/dimna21/Emotion_Recognition.mlflow\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cc04f7a6ee9c4808ba0828820b23a8af",
            "3dfb746bad1648229a483afbcf65eacd"
          ]
        },
        "id": "Q7aHFZyfz0k0",
        "outputId": "bf36a8b5-f1ea-4d85-b63b-334cb7709628"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting dagshub\n",
            "  Downloading dagshub-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mlflow-skinny==3.1.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (0.115.12)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (24.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (4.14.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (0.34.3)\n",
            "Collecting appdirs>=1.4.4 (from dagshub)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Collecting dacite~=1.6.0 (from dagshub)\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (9.1.2)\n",
            "Collecting gql[requests] (from dagshub)\n",
            "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting dataclasses-json (from dagshub)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting treelib>=1.6.4 (from dagshub)\n",
            "  Downloading treelib-1.7.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Collecting boto3 (from dagshub)\n",
            "  Downloading boto3-1.38.41-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting semver (from dagshub)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n",
            "  Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.0->mlflow) (4.0.12)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Collecting botocore<1.39.0,>=1.38.41 (from boto3->dagshub)\n",
            "  Downloading botocore-1.38.41-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->dagshub)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.0->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.0->mlflow) (3.4.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.0-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dagshub-0.5.10-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl (33 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading treelib-1.7.1-py3-none-any.whl (19 kB)\n",
            "Downloading boto3-1.38.41-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.38.41-py3-none-any.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: appdirs, treelib, semver, pathvalidate, mypy-extensions, marshmallow, jmespath, gunicorn, graphql-core, dacite, backoff, typing-inspect, opentelemetry-api, graphql-relay, gql, docker, botocore, alembic, s3transfer, opentelemetry-semantic-conventions, graphene, dataclasses-json, databricks-sdk, dagshub-annotation-converter, opentelemetry-sdk, boto3, mlflow-skinny, dagshub, mlflow\n",
            "Successfully installed alembic-1.16.2 appdirs-1.4.4 backoff-2.2.1 boto3-1.38.41 botocore-1.38.41 dacite-1.6.0 dagshub-0.5.10 dagshub-annotation-converter-0.1.10 databricks-sdk-0.57.0 dataclasses-json-0.6.7 docker-7.1.0 gql-3.5.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 jmespath-1.0.1 marshmallow-3.26.1 mlflow-3.1.0 mlflow-skinny-3.1.0 mypy-extensions-1.1.0 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pathvalidate-3.3.1 s3transfer-0.13.0 semver-3.0.4 treelib-1.7.1 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc04f7a6ee9c4808ba0828820b23a8af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=09774212-bad1-41d9-bffb-ebe1905c97ea&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=777b3ab261d0123bd1c46fc78103aabeab8913ffc3b4ab08d1d088c1d2840443\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as dimna21\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as dimna21\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"dimna21/Emotion_Recognition\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"dimna21/Emotion_Recognition\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository dimna21/Emotion_Recognition initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository dimna21/Emotion_Recognition initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow.keras\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    factor=0.5,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, lr_scheduler]\n",
        "\n",
        "# Start MLflow experiment for CNN\n",
        "with mlflow.start_run(run_name=\"cnn_speech_emotion_recognition\"):\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"model_type\": \"CNN\",\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"num_samples\": len(X_train),\n",
        "        \"num_features\": X_train.shape[1],\n",
        "        \"num_classes\": num_classes,\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"lr_scheduler_patience\": 3\n",
        "    })\n",
        "\n",
        "    # Create and compile CNN model\n",
        "    cnn_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    cnn_model.compile(\n",
        "        optimizer=cnn_optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train CNN model\n",
        "    print(\"Training CNN model...\")\n",
        "    cnn_history = cnn_model.fit(\n",
        "        X_train_reshaped, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_val_reshaped, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Log CNN metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"final_train_acc\": cnn_history.history['accuracy'][-1],\n",
        "        \"final_val_acc\": cnn_history.history['val_accuracy'][-1],\n",
        "        \"final_train_loss\": cnn_history.history['loss'][-1],\n",
        "        \"final_val_loss\": cnn_history.history['val_loss'][-1]\n",
        "    })\n",
        "\n",
        "    # Evaluate CNN on test set\n",
        "    cnn_pred_proba = cnn_model.predict(X_test_reshaped)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    cnn_pred = np.argmax(cnn_pred_proba, axis=1)\n",
        "\n",
        "    cnn_test_acc = accuracy_score(y_true, cnn_pred)\n",
        "    cnn_f1_weighted = f1_score(y_true, cnn_pred, average='weighted')\n",
        "    cnn_f1_per_class = f1_score(y_true, cnn_pred, average=None)\n",
        "\n",
        "    # Log CNN test results\n",
        "    mlflow.log_metrics({\n",
        "        \"test_accuracy\": cnn_test_acc,\n",
        "        \"test_f1_weighted\": cnn_f1_weighted\n",
        "    })\n",
        "\n",
        "    # Log per-emotion F1 scores\n",
        "    emotion_classes = le.classes_\n",
        "    cnn_f1_dict = {f\"f1_{emotion}\": score for emotion, score in zip(emotion_classes, cnn_f1_per_class)}\n",
        "    mlflow.log_metrics(cnn_f1_dict)\n",
        "\n",
        "    # Create CNN confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm_cnn = confusion_matrix(y_true, cnn_pred)\n",
        "    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('CNN Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('cnn_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('cnn_confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot CNN training history\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axes[0].plot(cnn_history.history['accuracy'], label='Train')\n",
        "    axes[0].plot(cnn_history.history['val_accuracy'], label='Val')\n",
        "    axes[0].set_title('CNN Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    axes[1].plot(cnn_history.history['loss'], label='Train')\n",
        "    axes[1].plot(cnn_history.history['val_loss'], label='Val')\n",
        "    axes[1].set_title('CNN Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('cnn_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('cnn_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Save CNN model\n",
        "    models_dir = '/content/drive/MyDrive/Model_Results'\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    cnn_model.save(f'{models_dir}/cnn_model.keras')\n",
        "    mlflow.log_artifact(f'{models_dir}/cnn_model.keras')\n",
        "\n",
        "    print(f\"CNN Training Complete!\")\n",
        "    print(f\"CNN Test Accuracy: {cnn_test_acc:.4f}, F1: {cnn_f1_weighted:.4f}\")"
      ],
      "metadata": {
        "id": "plb2paiNiY5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start MLflow experiment for BiLSTM\n",
        "with mlflow.start_run(run_name=\"bilstm_speech_emotion_recognition\"):\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"model_type\": \"CNN_BiLSTM\",\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"num_samples\": len(X_train),\n",
        "        \"num_features\": X_train.shape[1],\n",
        "        \"num_classes\": num_classes,\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"lr_scheduler_patience\": 3\n",
        "    })\n",
        "\n",
        "    # Create and compile BiLSTM model\n",
        "    bilstm_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "    bilstm_model.compile(\n",
        "        optimizer=bilstm_optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train BiLSTM model\n",
        "    print(\"Training CNN_BiLSTM model...\")\n",
        "    bilstm_history = bilstm_model.fit(\n",
        "        X_train_reshaped, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_val_reshaped, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Log BiLSTM metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"final_train_acc\": bilstm_history.history['accuracy'][-1],\n",
        "        \"final_val_acc\": bilstm_history.history['val_accuracy'][-1],\n",
        "        \"final_train_loss\": bilstm_history.history['loss'][-1],\n",
        "        \"final_val_loss\": bilstm_history.history['val_loss'][-1]\n",
        "    })\n",
        "\n",
        "    # Evaluate BiLSTM on test set\n",
        "    bilstm_pred_proba = bilstm_model.predict(X_test_reshaped)\n",
        "    bilstm_pred = np.argmax(bilstm_pred_proba, axis=1)\n",
        "\n",
        "    bilstm_test_acc = accuracy_score(y_true, bilstm_pred)\n",
        "    bilstm_f1_weighted = f1_score(y_true, bilstm_pred, average='weighted')\n",
        "    bilstm_f1_per_class = f1_score(y_true, bilstm_pred, average=None)\n",
        "\n",
        "    # Log BiLSTM test results\n",
        "    mlflow.log_metrics({\n",
        "        \"test_accuracy\": bilstm_test_acc,\n",
        "        \"test_f1_weighted\": bilstm_f1_weighted\n",
        "    })\n",
        "\n",
        "    # Log per-emotion F1 scores\n",
        "    bilstm_f1_dict = {f\"f1_{emotion}\": score for emotion, score in zip(emotion_classes, bilstm_f1_per_class)}\n",
        "    mlflow.log_metrics(bilstm_f1_dict)\n",
        "\n",
        "    # Create BiLSTM confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm_bilstm = confusion_matrix(y_true, bilstm_pred)\n",
        "    sns.heatmap(cm_bilstm, annot=True, fmt='d', cmap='Greens',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('CNN_BiLSTM Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('bilstm_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('bilstm_confusion_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot BiLSTM training history\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axes[0].plot(bilstm_history.history['accuracy'], label='Train')\n",
        "    axes[0].plot(bilstm_history.history['val_accuracy'], label='Val')\n",
        "    axes[0].set_title('CNN_BiLSTM Accuracy')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    axes[1].plot(bilstm_history.history['loss'], label='Train')\n",
        "    axes[1].plot(bilstm_history.history['val_loss'], label='Val')\n",
        "    axes[1].set_title('CNN_BiLSTM Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('bilstm_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('bilstm_training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Save BiLSTM model\n",
        "    bilstm_model.save(f'{models_dir}/bilstm_model.keras')\n",
        "    mlflow.log_artifact(f'{models_dir}/bilstm_model.keras')\n",
        "\n",
        "    print(f\"BiLSTM Training Complete!\")\n",
        "    print(f\"BiLSTM Test Accuracy: {bilstm_test_acc:.4f}, F1: {bilstm_f1_weighted:.4f}\")"
      ],
      "metadata": {
        "id": "-g9Cu5VricK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start MLflow experiment for Ensemble\n",
        "with mlflow.start_run(run_name=\"ensemble_speech_emotion_recognition\"):\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"model_type\": \"Ensemble_CNN_BiLSTM\",\n",
        "        \"ensemble_method\": \"averaging\",\n",
        "        \"num_samples\": len(X_train),\n",
        "        \"num_features\": X_train.shape[1],\n",
        "        \"num_classes\": num_classes\n",
        "    })\n",
        "\n",
        "    # Create ensemble predictions (averaging)\n",
        "    ensemble_pred_proba = (cnn_pred_proba + bilstm_pred_proba) / 2\n",
        "    ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
        "\n",
        "    # Calculate ensemble metrics\n",
        "    ensemble_test_acc = accuracy_score(y_true, ensemble_pred)\n",
        "    ensemble_f1_weighted = f1_score(y_true, ensemble_pred, average='weighted')\n",
        "    ensemble_f1_per_class = f1_score(y_true, ensemble_pred, average=None)\n",
        "\n",
        "    # Log ensemble test results\n",
        "    mlflow.log_metrics({\n",
        "        \"test_accuracy\": ensemble_test_acc,\n",
        "        \"test_f1_weighted\": ensemble_f1_weighted\n",
        "    })\n",
        "\n",
        "    # Log per-emotion F1 scores\n",
        "    ensemble_f1_dict = {f\"f1_{emotion}\": score for emotion, score in zip(emotion_classes, ensemble_f1_per_class)}\n",
        "    mlflow.log_metrics(ensemble_f1_dict)\n",
        "\n",
        "    # Create comprehensive comparison plots\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # CNN Confusion Matrix\n",
        "    plt.subplot(2, 3, 1)\n",
        "    cm_cnn = confusion_matrix(y_true, cnn_pred)\n",
        "    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('CNN Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # BiLSTM Confusion Matrix\n",
        "    plt.subplot(2, 3, 2)\n",
        "    cm_bilstm = confusion_matrix(y_true, bilstm_pred)\n",
        "    sns.heatmap(cm_bilstm, annot=True, fmt='d', cmap='Greens',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('CNN_BiLSTM Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Ensemble Confusion Matrix\n",
        "    plt.subplot(2, 3, 3)\n",
        "    cm_ensemble = confusion_matrix(y_true, ensemble_pred)\n",
        "    sns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Oranges',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('Ensemble Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # F1 Score Comparison Bar Plot\n",
        "    plt.subplot(2, 3, 4)\n",
        "    x_pos = np.arange(len(emotion_classes))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x_pos - width, cnn_f1_per_class, width, label='CNN', alpha=0.8)\n",
        "    plt.bar(x_pos, bilstm_f1_per_class, width, label='CNN_BiLSTM', alpha=0.8)\n",
        "    plt.bar(x_pos + width, ensemble_f1_per_class, width, label='Ensemble', alpha=0.8)\n",
        "\n",
        "    plt.xlabel('Emotions')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('Per-Emotion F1 Scores Comparison')\n",
        "    plt.xticks(x_pos, emotion_classes, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Overall Performance Comparison\n",
        "    plt.subplot(2, 3, 5)\n",
        "    models = ['CNN', 'CNN_BiLSTM', 'Ensemble']\n",
        "    accuracies = [cnn_test_acc, bilstm_test_acc, ensemble_test_acc]\n",
        "    f1_scores = [cnn_f1_weighted, bilstm_f1_weighted, ensemble_f1_weighted]\n",
        "\n",
        "    x_pos = np.arange(len(models))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x_pos - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
        "    plt.bar(x_pos + width/2, f1_scores, width, label='F1 Score', alpha=0.8)\n",
        "\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Overall Performance Comparison')\n",
        "    plt.xticks(x_pos, models)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ensemble_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('ensemble_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Create detailed classification reports\n",
        "    classification_reports = {\n",
        "        'cnn': classification_report(y_true, cnn_pred, target_names=emotion_classes, output_dict=True),\n",
        "        'bilstm': classification_report(y_true, bilstm_pred, target_names=emotion_classes, output_dict=True),\n",
        "        'ensemble': classification_report(y_true, ensemble_pred, target_names=emotion_classes, output_dict=True)\n",
        "    }\n",
        "\n",
        "    with open('classification_reports.json', 'w') as f:\n",
        "        json.dump(classification_reports, f, indent=2)\n",
        "    mlflow.log_artifact('classification_reports.json')\n",
        "\n",
        "    # Create summary metrics table\n",
        "    summary_data = {\n",
        "        'emotion': list(emotion_classes),\n",
        "        'cnn_f1': list(cnn_f1_per_class),\n",
        "        'bilstm_f1': list(bilstm_f1_per_class),\n",
        "        'ensemble_f1': list(ensemble_f1_per_class)\n",
        "    }\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv('per_emotion_f1_scores.csv', index=False)\n",
        "    mlflow.log_artifact('per_emotion_f1_scores.csv')\n",
        "\n",
        "    print(\"Ensemble Evaluation Complete!\")\n",
        "    print(f\"\\nFINAL RESULTS:\")\n",
        "    print(f\"CNN Test Accuracy: {cnn_test_acc:.4f}, F1: {cnn_f1_weighted:.4f}\")\n",
        "    print(f\"BiLSTM Test Accuracy: {bilstm_test_acc:.4f}, F1: {bilstm_f1_weighted:.4f}\")\n",
        "    print(f\"Ensemble Test Accuracy: {ensemble_test_acc:.4f}, F1: {ensemble_f1_weighted:.4f}\")"
      ],
      "metadata": {
        "id": "29UICMjQih4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow.keras\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# Callbacks from paper\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    factor=0.5,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks = [early_stopping, lr_scheduler]\n",
        "\n",
        "# Create separate optimizers for each model\n",
        "cnn_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "bilstm_optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "\n",
        "# Start MLflow experiment\n",
        "with mlflow.start_run(run_name=\"speech_emotion_recognition\"):\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_params({\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"num_samples\": len(X_train),\n",
        "        \"num_features\": X_train.shape[1],\n",
        "        \"num_classes\": num_classes,\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"lr_scheduler_patience\": 3\n",
        "    })\n",
        "\n",
        "    # Train CNN model\n",
        "    print(\"Training CNN model...\")\n",
        "    cnn_history = cnn_model.fit(\n",
        "        X_train_reshaped, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_val_reshaped, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Log CNN metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"cnn_final_train_acc\": cnn_history.history['accuracy'][-1],\n",
        "        \"cnn_final_val_acc\": cnn_history.history['val_accuracy'][-1],\n",
        "        \"cnn_final_train_loss\": cnn_history.history['loss'][-1],\n",
        "        \"cnn_final_val_loss\": cnn_history.history['val_loss'][-1]\n",
        "    })\n",
        "\n",
        "    # Train BiLSTM model\n",
        "    print(\"Training BiLSTM model...\")\n",
        "    bilstm_history = bilstm_model.fit(\n",
        "        X_train_reshaped, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_val_reshaped, y_val),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Log BiLSTM metrics\n",
        "    mlflow.log_metrics({\n",
        "        \"bilstm_final_train_acc\": bilstm_history.history['accuracy'][-1],\n",
        "        \"bilstm_final_val_acc\": bilstm_history.history['val_accuracy'][-1],\n",
        "        \"bilstm_final_train_loss\": bilstm_history.history['loss'][-1],\n",
        "        \"bilstm_final_val_loss\": bilstm_history.history['val_loss'][-1]\n",
        "    })\n",
        "\n",
        "    # Evaluate and log test results\n",
        "    cnn_pred_proba = cnn_model.predict(X_test_reshaped)\n",
        "    bilstm_pred_proba = bilstm_model.predict(X_test_reshaped)\n",
        "    ensemble_pred_proba = (cnn_pred_proba + bilstm_pred_proba) / 2\n",
        "\n",
        "    # Convert predictions and true labels\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    cnn_pred = np.argmax(cnn_pred_proba, axis=1)\n",
        "    bilstm_pred = np.argmax(bilstm_pred_proba, axis=1)\n",
        "    ensemble_pred = np.argmax(ensemble_pred_proba, axis=1)\n",
        "\n",
        "    # Calculate test accuracies\n",
        "    cnn_test_acc = accuracy_score(y_true, cnn_pred)\n",
        "    bilstm_test_acc = accuracy_score(y_true, bilstm_pred)\n",
        "    ensemble_test_acc = accuracy_score(y_true, ensemble_pred)\n",
        "\n",
        "    # Calculate weighted F1 scores\n",
        "    cnn_f1_weighted = f1_score(y_true, cnn_pred, average='weighted')\n",
        "    bilstm_f1_weighted = f1_score(y_true, bilstm_pred, average='weighted')\n",
        "    ensemble_f1_weighted = f1_score(y_true, ensemble_pred, average='weighted')\n",
        "\n",
        "    # Calculate per-class F1 scores\n",
        "    cnn_f1_per_class = f1_score(y_true, cnn_pred, average=None)\n",
        "    bilstm_f1_per_class = f1_score(y_true, bilstm_pred, average=None)\n",
        "    ensemble_f1_per_class = f1_score(y_true, ensemble_pred, average=None)\n",
        "\n",
        "    # Log test accuracies and weighted F1 scores\n",
        "    mlflow.log_metrics({\n",
        "        \"cnn_test_accuracy\": cnn_test_acc,\n",
        "        \"bilstm_test_accuracy\": bilstm_test_acc,\n",
        "        \"ensemble_test_accuracy\": ensemble_test_acc,\n",
        "        \"cnn_f1_weighted\": cnn_f1_weighted,\n",
        "        \"bilstm_f1_weighted\": bilstm_f1_weighted,\n",
        "        \"ensemble_f1_weighted\": ensemble_f1_weighted\n",
        "    })\n",
        "\n",
        "    # Log per-emotion F1 scores for each model\n",
        "    emotion_classes = le.classes_\n",
        "\n",
        "    # CNN per-emotion F1 scores\n",
        "    cnn_f1_dict = {f\"cnn_f1_{emotion}\": score for emotion, score in zip(emotion_classes, cnn_f1_per_class)}\n",
        "    mlflow.log_metrics(cnn_f1_dict)\n",
        "\n",
        "    # BiLSTM per-emotion F1 scores\n",
        "    bilstm_f1_dict = {f\"bilstm_f1_{emotion}\": score for emotion, score in zip(emotion_classes, bilstm_f1_per_class)}\n",
        "    mlflow.log_metrics(bilstm_f1_dict)\n",
        "\n",
        "    # Ensemble per-emotion F1 scores\n",
        "    ensemble_f1_dict = {f\"ensemble_f1_{emotion}\": score for emotion, score in zip(emotion_classes, ensemble_f1_per_class)}\n",
        "    mlflow.log_metrics(ensemble_f1_dict)\n",
        "\n",
        "    # Create and save confusion matrices\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # CNN Confusion Matrix\n",
        "    plt.subplot(2, 3, 1)\n",
        "    cm_cnn = confusion_matrix(y_true, cnn_pred)\n",
        "    sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('CNN Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # BiLSTM Confusion Matrix\n",
        "    plt.subplot(2, 3, 2)\n",
        "    cm_bilstm = confusion_matrix(y_true, bilstm_pred)\n",
        "    sns.heatmap(cm_bilstm, annot=True, fmt='d', cmap='Greens',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('BiLSTM Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # Ensemble Confusion Matrix\n",
        "    plt.subplot(2, 3, 3)\n",
        "    cm_ensemble = confusion_matrix(y_true, ensemble_pred)\n",
        "    sns.heatmap(cm_ensemble, annot=True, fmt='d', cmap='Oranges',\n",
        "                xticklabels=emotion_classes, yticklabels=emotion_classes)\n",
        "    plt.title('Ensemble Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "    # F1 Score Comparison Bar Plot\n",
        "    plt.subplot(2, 3, 4)\n",
        "    x_pos = np.arange(len(emotion_classes))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x_pos - width, cnn_f1_per_class, width, label='CNN', alpha=0.8)\n",
        "    plt.bar(x_pos, bilstm_f1_per_class, width, label='BiLSTM', alpha=0.8)\n",
        "    plt.bar(x_pos + width, ensemble_f1_per_class, width, label='Ensemble', alpha=0.8)\n",
        "\n",
        "    plt.xlabel('Emotions')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('Per-Emotion F1 Scores Comparison')\n",
        "    plt.xticks(x_pos, emotion_classes, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Overall Performance Comparison\n",
        "    plt.subplot(2, 3, 5)\n",
        "    models = ['CNN', 'BiLSTM', 'Ensemble']\n",
        "    accuracies = [cnn_test_acc, bilstm_test_acc, ensemble_test_acc]\n",
        "    f1_scores = [cnn_f1_weighted, bilstm_f1_weighted, ensemble_f1_weighted]\n",
        "\n",
        "    x_pos = np.arange(len(models))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x_pos - width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
        "    plt.bar(x_pos + width/2, f1_scores, width, label='F1 Score', alpha=0.8)\n",
        "\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Overall Performance Comparison')\n",
        "    plt.xticks(x_pos, models)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices_and_metrics.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('confusion_matrices_and_metrics.png')\n",
        "\n",
        "    # Save and log models\n",
        "    cnn_model.save('cnn_model.h5')\n",
        "    bilstm_model.save('bilstm_model.h5')\n",
        "\n",
        "    # Log the saved model files as artifacts\n",
        "    mlflow.log_artifact('cnn_model.h5')\n",
        "    mlflow.log_artifact('bilstm_model.h5')\n",
        "\n",
        "    # Log training plots as artifacts\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # CNN plots\n",
        "    axes[0,0].plot(cnn_history.history['accuracy'], label='Train')\n",
        "    axes[0,0].plot(cnn_history.history['val_accuracy'], label='Val')\n",
        "    axes[0,0].set_title('CNN Accuracy')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True)\n",
        "\n",
        "    axes[0,1].plot(cnn_history.history['loss'], label='Train')\n",
        "    axes[0,1].plot(cnn_history.history['val_loss'], label='Val')\n",
        "    axes[0,1].set_title('CNN Loss')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True)\n",
        "\n",
        "    # BiLSTM plots\n",
        "    axes[1,0].plot(bilstm_history.history['accuracy'], label='Train')\n",
        "    axes[1,0].plot(bilstm_history.history['val_accuracy'], label='Val')\n",
        "    axes[1,0].set_title('BiLSTM Accuracy')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True)\n",
        "\n",
        "    axes[1,1].plot(bilstm_history.history['loss'], label='Train')\n",
        "    axes[1,1].plot(bilstm_history.history['val_loss'], label='Val')\n",
        "    axes[1,1].set_title('BiLSTM Loss')\n",
        "    axes[1,1].legend()\n",
        "    axes[1,1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('training_history.png')\n",
        "\n",
        "    # Create detailed classification reports and save as JSON\n",
        "    classification_reports = {\n",
        "        'cnn': classification_report(y_true, cnn_pred, target_names=emotion_classes, output_dict=True),\n",
        "        'bilstm': classification_report(y_true, bilstm_pred, target_names=emotion_classes, output_dict=True),\n",
        "        'ensemble': classification_report(y_true, ensemble_pred, target_names=emotion_classes, output_dict=True)\n",
        "    }\n",
        "\n",
        "    with open('classification_reports.json', 'w') as f:\n",
        "        json.dump(classification_reports, f, indent=2)\n",
        "    mlflow.log_artifact('classification_reports.json')\n",
        "\n",
        "    # Create summary metrics table\n",
        "    summary_data = {\n",
        "        'emotion': list(emotion_classes),\n",
        "        'cnn_f1': list(cnn_f1_per_class),\n",
        "        'bilstm_f1': list(bilstm_f1_per_class),\n",
        "        'ensemble_f1': list(ensemble_f1_per_class)\n",
        "    }\n",
        "\n",
        "    import pandas as pd\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    summary_df.to_csv('per_emotion_f1_scores.csv', index=False)\n",
        "    mlflow.log_artifact('per_emotion_f1_scores.csv')\n",
        "\n",
        "    print(\"Experiment logged to DagsHub!\")\n",
        "    print(f\"CNN Test Accuracy: {cnn_test_acc:.4f}, F1: {cnn_f1_weighted:.4f}\")\n",
        "    print(f\"BiLSTM Test Accuracy: {bilstm_test_acc:.4f}, F1: {bilstm_f1_weighted:.4f}\")\n",
        "    print(f\"Ensemble Test Accuracy: {ensemble_test_acc:.4f}, F1: {ensemble_f1_weighted:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "VjHuJo200zK3",
        "outputId": "d27f6daf-1ee2-4b8b-fbda-b6c9c4da36c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training CNN model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 664ms/step - accuracy: 0.4379 - loss: 1.6693 - val_accuracy: 0.1498 - val_loss: 2.8154 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 646ms/step - accuracy: 0.6417 - loss: 0.9697 - val_accuracy: 0.1531 - val_loss: 2.7730 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 634ms/step - accuracy: 0.7145 - loss: 0.7942 - val_accuracy: 0.2041 - val_loss: 3.0740 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 643ms/step - accuracy: 0.7782 - loss: 0.6247 - val_accuracy: 0.2790 - val_loss: 2.5800 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 643ms/step - accuracy: 0.8288 - loss: 0.5018 - val_accuracy: 0.3967 - val_loss: 2.0809 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 643ms/step - accuracy: 0.8415 - loss: 0.4662 - val_accuracy: 0.5679 - val_loss: 1.3441 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 635ms/step - accuracy: 0.8210 - loss: 0.5073 - val_accuracy: 0.6066 - val_loss: 1.2749 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 635ms/step - accuracy: 0.8952 - loss: 0.3253 - val_accuracy: 0.6041 - val_loss: 1.2154 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 641ms/step - accuracy: 0.9156 - loss: 0.2648 - val_accuracy: 0.6107 - val_loss: 1.2146 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 638ms/step - accuracy: 0.9348 - loss: 0.2210 - val_accuracy: 0.5926 - val_loss: 1.3208 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 636ms/step - accuracy: 0.9329 - loss: 0.2249 - val_accuracy: 0.6041 - val_loss: 1.4688 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 640ms/step - accuracy: 0.9674 - loss: 0.1379 - val_accuracy: 0.6206 - val_loss: 1.3696 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 659ms/step - accuracy: 0.9696 - loss: 0.1190 - val_accuracy: 0.6214 - val_loss: 1.4193 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 641ms/step - accuracy: 0.9863 - loss: 0.0770 - val_accuracy: 0.6247 - val_loss: 1.4366 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "🏃 View run speech_emotion_recognition at: https://dagshub.com/dimna21/Emotion_Recognition.mlflow/#/experiments/0/runs/154566a8cad14c57ab3488f3675f10f4\n",
            "🧪 View experiment at: https://dagshub.com/dimna21/Emotion_Recognition.mlflow/#/experiments/0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-1941024490.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Train CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training CNN model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     cnn_history = cnn_model.fit(\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mX_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "models_dir = '/content/drive/MyDrive/Model_Results'\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "print(\"Saving models to Google Drive...\")\n",
        "cnn_model.save(f'{models_dir}/cnn_model.h5')\n",
        "bilstm_model.save(f'{models_dir}/bilstm_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVyIrkdv3m7q",
        "outputId": "18889f05-40bf-405a-fef9-2aefc728571b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving models to Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}